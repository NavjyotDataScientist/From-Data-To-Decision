{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Author:** Navjyot Architect  \n",
        "**Email:** navjyot.dataarchitect@proton.me  \n",
        "\n",
        "Hello there! Excellent question, Captain! Moving from isolated Colab notebooks to a structured, professional project in VS Code is a  **critical step**  in becoming a true data science or ML engineer. It's all about reproducibility, collaboration, maintainability, and scalability.\n",
        "\n",
        "You're absolutely right to focus on  `requirements.txt`,  `yaml`  configs,  `gitignore`, and a proper folder structure. This is how real-world ML projects are built.\n",
        "\n",
        "Let's break down the conceptual steps first, then we'll map them to a concrete project structure using a mixed numerical and categorical dataset like  **Customer Churn Prediction**.\n",
        "\n",
        "----------\n",
        "\n",
        "## Part 1: Conceptual Framework - The Professional ML Project Lifecycle\n",
        "\n",
        "When professionals build ML projects, they don't just dump code into one file. They follow a well-defined process and structure to ensure robustness.\n",
        "\n",
        "### 1. The \"Why\" - Beyond Colab\n",
        "\n",
        "-   **Reproducibility:**  Anyone (including your future self) should be able to set up the project and get the same results.\n",
        "-   **Maintainability:**  Code is modular, easy to update, and debug.\n",
        "-   **Collaboration:**  Multiple people can work on different parts without conflict.\n",
        "-   **Scalability:**  Easier to add new features, models, or integrate with other systems.\n",
        "-   **Version Control:**  Track changes, revert mistakes, and collaborate effectively using Git.\n",
        "-   **Deployment Readiness:**  A structured project is a prerequisite for deploying your model as an API or service.\n",
        "\n",
        "### 2. Core Phases of an ML Project (Professional Lens)\n",
        "\n",
        "Here's how the typical ML phases translate into a structured project:\n",
        "\n",
        "#### Phase 0: Project Setup & Environment Management\n",
        "\n",
        "-   **Concept:**  Before writing any ML code, you set up the project folder, define dependencies, and create an isolated environment. This prevents \"it works on my machine\" problems.\n",
        "-   **Key Files/Folders:**\n",
        "    -   `project_root/`: The main project directory.\n",
        "    -   `venv/`  or  `.conda/`: Your isolated virtual environment.\n",
        "    -   `requirements.txt`: Lists all Python packages and their exact versions.\n",
        "    -   `.gitignore`: Specifies files/folders Git should ignore (e.g.,  `venv/`, data, models).\n",
        "    -   `README.md`: Project description, setup instructions, how to run.\n",
        "    -   `config.yaml`  (or  `config.ini`,  `.env`): Centralized place for configuration parameters (e.g., data paths, model hyperparameters, database credentials).\n",
        "\n",
        "#### Phase 1: Data Ingestion & Understanding\n",
        "\n",
        "-   **Concept:**  Obtaining the raw data, storing it, and performing an initial inspection to understand its structure, types, and potential issues.\n",
        "-   **Key Files/Folders:**\n",
        "    -   `data/raw/`: Where original, unaltered datasets are stored.\n",
        "    -   `src/data/make_dataset.py`: Script to download data from external sources, or load from raw and perform initial sanity checks.\n",
        "    -   `notebooks/01_data_understanding.ipynb`: A Jupyter notebook for interactive exploration, generating initial statistics and visualizations.  _Crucially, these notebooks are for exploration, not for production code._\n",
        "\n",
        "#### Phase 2: Data Cleaning & Preprocessing\n",
        "\n",
        "-   **Concept:**  Handling missing values, outliers, correcting data types, dealing with inconsistent entries, and transforming raw features into a format suitable for ML models (e.g., encoding categorical variables, scaling numerical features).\n",
        "-   **Key Files/Folders:**\n",
        "    -   `data/interim/`: Intermediate datasets generated during cleaning.\n",
        "    -   `data/processed/`: The final, clean dataset ready for modeling.\n",
        "    -   `src/data/make_dataset.py`: Might also contain cleaning logic, or call helper functions.\n",
        "    -   `src/features/build_features.py`: Contains functions/classes for feature engineering (e.g., one-hot encoding, standard scaling, creating new features).\n",
        "    -   `src/utils/`: Helper functions used across different scripts.\n",
        "    -   `notebooks/02_data_cleaning_and_eda.ipynb`: Interactive notebook to experiment with cleaning techniques and perform detailed EDA.\n",
        "\n",
        "#### Phase 3: Exploratory Data Analysis (EDA)\n",
        "\n",
        "-   **Concept:**  Deep dive into the processed data to uncover patterns, relationships, and insights. This informs model selection and further feature engineering.\n",
        "-   **Key Files/Folders:**\n",
        "    -   `notebooks/02_data_cleaning_and_eda.ipynb`: Continued interactive analysis, generating plots.\n",
        "    -   `reports/figures/`: Directory to save important charts and visualizations.\n",
        "    -   `reports/eda_report.md`  (optional): A markdown file summarizing key EDA findings.\n",
        "\n",
        "#### Phase 4: Model Development & Training\n",
        "\n",
        "-   **Concept:**  Selecting appropriate ML algorithms, splitting data, training models, and tuning hyperparameters.\n",
        "-   **Key Files/Folders:**\n",
        "    -   `src/models/train_model.py`: The main script for training the ML model. It orchestrates data loading, preprocessing (calling  `build_features.py`), model instantiation, training, and evaluation.\n",
        "    -   `src/models/predict_model.py`: A script for making predictions using a trained model.\n",
        "    -   `models/`: Directory to save trained model artifacts (e.g.,  `model.pkl`,  `scaler.pkl`).\n",
        "    -   `notebooks/03_model_training_and_evaluation.ipynb`: For rapid prototyping and experimenting with different models/hyperparameters before formalizing in scripts.\n",
        "\n",
        "#### Phase 5: Model Evaluation & Testing\n",
        "\n",
        "-   **Concept:**  Objectively assessing model performance on unseen data using appropriate metrics (accuracy, precision, recall, F1, RMSE, etc.) and ensuring generalization.\n",
        "-   **Key Files/Folders:**\n",
        "    -   Part of  `src/models/train_model.py`: Evaluation metrics are calculated and logged here.\n",
        "    -   `reports/results/`: Directory to store evaluation metrics (e.g.,  `metrics.json`,  `evaluation_report.txt`).\n",
        "    -   `reports/mlruns/`  (if using MLflow): For experiment tracking.\n",
        "\n",
        "#### Phase 6: Model Deployment (Future Step)\n",
        "\n",
        "-   **Concept:**  Making the trained model available for predictions in a production environment (e.g., a web API, batch processing).\n",
        "-   **Key Files/Folders:**\n",
        "    -   `src/app/api.py`: (e.g., Flask/FastAPI app for serving predictions).\n",
        "    -   `Dockerfile`: For containerizing the application.\n",
        "    -   `requirements.txt`: (Used by Docker).\n",
        "\n",
        "### 3. Essential Tools & Practices in VS Code\n",
        "\n",
        "-   **Integrated Terminal:**  Run commands (`pip install`,  `python script.py`,  `git`).\n",
        "-   **Python Extension:**  Provides IntelliSense, linting, debugging, environment selection.\n",
        "-   **Jupyter Support:**  Open and run  `.ipynb`  files directly in VS Code.\n",
        "-   **Git Integration:**  Source control directly within the IDE.\n",
        "-   **YAML Extension:**  Syntax highlighting for  `config.yaml`.\n",
        "-   **Virtual Environments:**  Always use a  `venv`  or  `conda`  environment to isolate project dependencies. VS Code makes it easy to select and activate these.\n",
        "-   **Linting (e.g., Black, Flake8):**  Automatically formats code and checks for style issues.\n",
        "\n",
        "----------\n",
        "\n",
        "## Part 2: Concrete Example - Customer Churn Prediction Project\n",
        "\n",
        "Let's apply these concepts to a  **Customer Churn Prediction**  project. This dataset typically includes a mix of numerical features (e.g., tenure, monthly charges) and categorical features (e.g., gender, contract type, internet service).\n",
        "\n",
        "**Goal:**  Predict whether a customer will churn (cancel their service) or not.\n",
        "\n",
        "### Step 1: Project Setup in VS Code\n",
        "\n",
        "1.  **Create Project Directory:**\n",
        "    \n",
        "    -   Open your terminal (or VS Code integrated terminal).\n",
        "    -   `mkdir customer_churn_prediction`\n",
        "    -   `cd customer_churn_prediction`\n",
        "    -   `code .`  (This opens VS Code in your new project directory).\n",
        "2.  **Initialize Virtual Environment:**\n",
        "    \n",
        "    -   In the VS Code terminal:  `python -m venv venv`\n",
        "    -   Activate it:\n",
        "        -   Windows:  `.\\venv\\Scripts\\activate`\n",
        "        -   macOS/Linux:  `source venv/bin/activate`\n",
        "    -   **VS Code Tip:**  VS Code will usually detect the new  `venv`  and ask if you want to select it as your interpreter. Always say yes. If not, open Command Palette (`Ctrl+Shift+P`  or  `Cmd+Shift+P`), type \"Python: Select Interpreter\", and choose the one inside your  `venv`  folder.\n",
        "3.  **Create  `requirements.txt`:**\n",
        "    \n",
        "    -   Create a file named  `requirements.txt`  in the root of your project.\n",
        "    -   Add initial dependencies:\n",
        "        \n",
        "        ```\n",
        "        pandas\n",
        "        scikit-learn\n",
        "        matplotlib\n",
        "        seaborn\n",
        "        jupyter # To run notebooks in VS Code\n",
        "        ipykernel # Required by jupyter\n",
        "        pyyaml # For config.yaml\n",
        "        \n",
        "        ```\n",
        "        \n",
        "    -   Install them:  `pip install -r requirements.txt`\n",
        "4.  **Create  `.gitignore`:**\n",
        "    \n",
        "    -   Create a file named  `.gitignore`  in the root of your project.\n",
        "    -   Add common ignores:\n",
        "        \n",
        "        ```\n",
        "        # Virtual environment\n",
        "        venv/\n",
        "        .venv/\n",
        "        \n",
        "        # Data\n",
        "        data/raw/\n",
        "        data/interim/\n",
        "        data/processed/\n",
        "        \n",
        "        # Models\n",
        "        models/\n",
        "        \n",
        "        # Jupyter notebooks outputs\n",
        "        .ipynb_checkpoints/\n",
        "        *.pyc\n",
        "        __pycache__/\n",
        "        \n",
        "        # VS Code specific\n",
        "        .vscode/\n",
        "        \n",
        "        # Environment variables\n",
        "        .env\n",
        "        \n",
        "        ```\n",
        "        \n",
        "    -   _Explanation:_  We ignore  `venv/`  because it's locally generated. We ignore data and models because they can be large and are usually tracked separately or regenerated by scripts.\n",
        "5.  **Create  `README.md`:**\n",
        "    \n",
        "    -   Create a file named  `README.md`  in the root.\n",
        "    -   Add basic project description, setup instructions, and how to run.\n",
        "        \n",
        "        ```\n",
        "        # Customer Churn Prediction Project\n",
        "        \n",
        "        This project aims to predict customer churn using a mixed dataset of numerical and categorical features.\n",
        "        \n",
        "        ## Setup\n",
        "        \n",
        "        1.  Clone the repository: `git clone <repository_url>`\n",
        "        2.  Navigate to the project directory: `cd customer_churn_prediction`\n",
        "        3.  Create a virtual environment: `python -m venv venv`\n",
        "        4.  Activate the environment:\n",
        "            *   Windows: `.\\venv\\Scripts\\activate`\n",
        "            *   macOS/Linux: `source venv/bin/activate`\n",
        "        5.  Install dependencies: `pip install -r requirements.txt`\n",
        "        \n",
        "        ## Data\n",
        "        \n",
        "        The raw data is expected to be in `data/raw/customer_churn.csv`.\n",
        "        (You'll need to download this from a source like Kaggle or provide a placeholder.)\n",
        "        \n",
        "        ## How to Run\n",
        "        \n",
        "        1.  **Explore Data:** Open `notebooks/01_data_understanding.ipynb` and `notebooks/02_data_cleaning_and_eda.ipynb`\n",
        "        2.  **Process Data:** `python src/data/make_dataset.py`\n",
        "        3.  **Train Model:** `python src/models/train_model.py`\n",
        "        4.  **Predict:** `python src/models/predict_model.py --input_data <path_to_new_data>`\n",
        "        \n",
        "        ## Project Structure\n",
        "        \n",
        "        ```\n",
        "        \n",
        "        . ├── data/ │ ├── raw/ │ ├── interim/ │ └── processed/ ├── notebooks/ ├── src/ │ ├──  **init**.py │ ├── data/ │ │ └── make_dataset.py │ ├── features/ │ │ └── build_features.py │ ├── models/ │ │ ├── train_model.py │ │ └── predict_model.py │ └── utils/ │ └── helpers.py ├── models/ ├── reports/ │ ├── figures/ │ └── results/ ├── conf/ │ └── config.yaml ├── venv/ ├── .gitignore ├── requirements.txt └── README.md\n",
        "        \n",
        "6.  **Create  `config.yaml`:**\n",
        "    \n",
        "    -   Create a folder  `conf/`  in the root.\n",
        "    -   Inside  `conf/`, create  `config.yaml`.\n",
        "    -   Add configurations:\n",
        "        \n",
        "        ```\n",
        "        # Data paths\n",
        "        data_paths:\n",
        "          raw: \"data/raw/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
        "          processed: \"data/processed/processed_churn_data.csv\"\n",
        "          interim: \"data/interim/cleaned_churn_data.csv\"\n",
        "        \n",
        "        # Feature engineering parameters\n",
        "        features:\n",
        "          categorical_features:\n",
        "            - \"gender\"\n",
        "            - \"Partner\"\n",
        "            - \"Dependents\"\n",
        "            - \"PhoneService\"\n",
        "            - \"MultipleLines\"\n",
        "            - \"InternetService\"\n",
        "            - \"OnlineSecurity\"\n",
        "            - \"OnlineBackup\"\n",
        "            - \"DeviceProtection\"\n",
        "            - \"TechSupport\"\n",
        "            - \"StreamingTV\"\n",
        "            - \"StreamingMovies\"\n",
        "            - \"Contract\"\n",
        "            - \"PaperlessBilling\"\n",
        "            - \"PaymentMethod\"\n",
        "          numerical_features:\n",
        "            - \"tenure\"\n",
        "            - \"MonthlyCharges\"\n",
        "            - \"TotalCharges\"\n",
        "          target_feature: \"Churn\"\n",
        "          features_to_drop: # Example of columns to drop during preprocessing\n",
        "            - \"customerID\"\n",
        "        \n",
        "        # Model parameters\n",
        "        model_params:\n",
        "          random_state: 42\n",
        "          test_size: 0.2\n",
        "          # Logistic Regression example\n",
        "          logistic_regression:\n",
        "            C: 0.1\n",
        "            solver: \"liblinear\"\n",
        "            max_iter: 1000\n",
        "          # RandomForestClassifier example\n",
        "          random_forest:\n",
        "            n_estimators: 100\n",
        "            max_depth: 10\n",
        "            min_samples_split: 5\n",
        "        \n",
        "        # Output paths\n",
        "        output_paths:\n",
        "          model_dir: \"models/\"\n",
        "          scaler_path: \"models/scaler.pkl\"\n",
        "          model_path: \"models/logistic_regression_model.pkl\"\n",
        "          metrics_path: \"reports/results/metrics.json\"\n",
        "          figures_dir: \"reports/figures/\"\n",
        "        \n",
        "        ```\n",
        "        \n",
        "    -   _Explanation:_  This centralizes all parameters. Scripts will read from this, making it easy to change settings without altering code.\n",
        "7.  **Create Project Folder Structure:**\n",
        "    \n",
        "    -   Manually create the directories defined in your  `README.md`  and  `config.yaml`:\n",
        "        \n",
        "        ```\n",
        "        .\n",
        "        ├── conf/\n",
        "        │   └── config.yaml\n",
        "        ├── data/\n",
        "        │   ├── raw/\n",
        "        │   ├── interim/\n",
        "        │   └── processed/\n",
        "        ├── models/\n",
        "        ├── notebooks/\n",
        "        ├── reports/\n",
        "        │   ├── figures/\n",
        "        │   └── results/\n",
        "        ├── src/\n",
        "        │   ├── data/\n",
        "        │   ├── features/\n",
        "        │   ├── models/\n",
        "        │   └── utils/\n",
        "        ├── .gitignore\n",
        "        ├── README.md\n",
        "        └── requirements.txt\n",
        "        \n",
        "        ```\n",
        "        \n",
        "    -   Inside  `src/`, create an empty  `__init__.py`  file in  `src/`,  `src/data/`,  `src/features/`,  `src/models/`,  `src/utils/`  to make them Python packages.\n",
        "        -   e.g.,  `touch src/__init__.py`,  `touch src/data/__init__.py`, etc.\n",
        "\n",
        "### Step 2: Data Acquisition & Initial Exploration (using  `notebooks/01_data_understanding.ipynb`)\n",
        "\n",
        "1.  **Download Data:**  Find a \"Telco Customer Churn\" dataset (e.g., from Kaggle) and place it in  `data/raw/WA_Fn-UseC_-Telco-Customer-Churn.csv`.\n",
        "    \n",
        "2.  **Create  `notebooks/01_data_understanding.ipynb`:**\n",
        "    \n",
        "    -   Open  `notebooks/01_data_understanding.ipynb`  in VS Code.\n",
        "    -   Load the config:\n",
        "        \n",
        "        ```\n",
        "        import yaml\n",
        "        import pandas as pd\n",
        "        \n",
        "        # Load configuration\n",
        "        with open('conf/config.yaml', 'r') as f:\n",
        "            config = yaml.safe_load(f)\n",
        "        \n",
        "        RAW_DATA_PATH = config['data_paths']['raw']\n",
        "        \n",
        "        # Load data\n",
        "        df = pd.read_csv(RAW_DATA_PATH)\n",
        "        \n",
        "        # Initial inspection\n",
        "        print(df.head())\n",
        "        print(df.info())\n",
        "        print(df.describe(include='all'))\n",
        "        print(df.isnull().sum())\n",
        "        \n",
        "        ```\n",
        "        \n",
        "    -   _Purpose:_  This notebook is for quick checks, data types, missing values, and getting a feel for the data. No heavy cleaning or modeling here.\n",
        "\n",
        "### Step 3: Data Cleaning & Preprocessing (Scripted -  `src/data/make_dataset.py`,  `src/features/build_features.py`)\n",
        "\n",
        "This is where you move from interactive exploration to production-ready code.\n",
        "\n",
        "1.  **Create  `src/utils/helpers.py`:**  For reusable functions.\n",
        "    \n",
        "    ```\n",
        "    # src/utils/helpers.py\n",
        "    import yaml\n",
        "    \n",
        "    def load_config(config_path='conf/config.yaml'):\n",
        "        with open(config_path, 'r') as f:\n",
        "            return yaml.safe_load(f)\n",
        "    \n",
        "    def save_dataframe(df, path):\n",
        "        df.to_csv(path, index=False)\n",
        "        print(f\"DataFrame saved to {path}\")\n",
        "    \n",
        "    ```\n",
        "    \n",
        "2.  **Create  `src/data/make_dataset.py`:**\n",
        "    \n",
        "    ```\n",
        "    # src/data/make_dataset.py\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from src.utils.helpers import load_config, save_dataframe\n",
        "    \n",
        "    def clean_data(df):\n",
        "        # Handle 'No internet service' and 'No phone service' categories\n",
        "        for col in ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',\n",
        "                    'StreamingTV', 'StreamingMovies', 'MultipleLines']:\n",
        "            df[col] = df[col].replace('No internet service', 'No')\n",
        "            if col == 'MultipleLines':\n",
        "                df[col] = df[col].replace('No phone service', 'No')\n",
        "    \n",
        "        # Convert TotalCharges to numeric, coercing errors to NaN\n",
        "        df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
        "    \n",
        "        # Handle missing TotalCharges (often a few rows, impute with median or drop)\n",
        "        # For simplicity, let's drop rows with missing TotalCharges for now\n",
        "        df.dropna(subset=['TotalCharges'], inplace=True)\n",
        "    \n",
        "        # Drop customerID (as per config, if present)\n",
        "        config = load_config()\n",
        "        if 'customerID' in df.columns and 'customerID' in config['features']['features_to_drop']:\n",
        "            df = df.drop('customerID', axis=1)\n",
        "    \n",
        "        # Convert target variable 'Churn' to numerical (Yes/No to 1/0)\n",
        "        df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})\n",
        "    \n",
        "        return df\n",
        "    \n",
        "    def main():\n",
        "        config = load_config()\n",
        "        raw_data_path = config['data_paths']['raw']\n",
        "        interim_data_path = config['data_paths']['interim']\n",
        "    \n",
        "        print(f\"Loading raw data from {raw_data_path}...\")\n",
        "        df = pd.read_csv(raw_data_path)\n",
        "    \n",
        "        print(\"Cleaning data...\")\n",
        "        cleaned_df = clean_data(df.copy()) # Use a copy to avoid SettingWithCopyWarning\n",
        "    \n",
        "        save_dataframe(cleaned_df, interim_data_path)\n",
        "        print(\"Data cleaning complete. Interim data saved.\")\n",
        "    \n",
        "    if __name__ == \"__main__\":\n",
        "        main()\n",
        "    \n",
        "    ```\n",
        "    \n",
        "    -   _Run:_  `python src/data/make_dataset.py`\n",
        "3.  **Create  `src/features/build_features.py`:**\n",
        "    \n",
        "    ```\n",
        "    # src/features/build_features.py\n",
        "    import pandas as pd\n",
        "    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "    from sklearn.compose import ColumnTransformer\n",
        "    from sklearn.pipeline import Pipeline\n",
        "    import joblib # For saving/loading preprocessors\n",
        "    from src.utils.helpers import load_config, save_dataframe\n",
        "    \n",
        "    def build_preprocessor(df, config):\n",
        "        categorical_features = config['features']['categorical_features']\n",
        "        numerical_features = config['features']['numerical_features']\n",
        "        target_feature = config['features']['target_feature']\n",
        "    \n",
        "        # Ensure features exist in dataframe and filter target\n",
        "        categorical_features = [f for f in categorical_features if f in df.columns and f != target_feature]\n",
        "        numerical_features = [f for f in numerical_features if f in df.columns and f != target_feature]\n",
        "    \n",
        "        numerical_transformer = StandardScaler()\n",
        "        categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "    \n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numerical_transformer, numerical_features),\n",
        "                ('cat', categorical_transformer, categorical_features)\n",
        "            ],\n",
        "            remainder='passthrough' # Keep other columns (like target)\n",
        "        )\n",
        "        return preprocessor\n",
        "    \n",
        "    def main():\n",
        "        config = load_config()\n",
        "        interim_data_path = config['data_paths']['interim']\n",
        "        processed_data_path = config['data_paths']['processed']\n",
        "        scaler_path = config['output_paths']['scaler_path']\n",
        "        target_feature = config['features']['target_feature']\n",
        "    \n",
        "        print(f\"Loading interim data from {interim_data_path}...\")\n",
        "        df_interim = pd.read_csv(interim_data_path)\n",
        "    \n",
        "        # Separate features and target before fitting preprocessor\n",
        "        X = df_interim.drop(columns=[target_feature])\n",
        "        y = df_interim[target_feature]\n",
        "    \n",
        "        print(\"Building preprocessor and transforming features...\")\n",
        "        preprocessor = build_preprocessor(X, config)\n",
        "        X_processed = preprocessor.fit_transform(X)\n",
        "    \n",
        "        # Get feature names after one-hot encoding\n",
        "        # This is a bit tricky with ColumnTransformer. One way is:\n",
        "        # Get names for numerical features\n",
        "        num_features_transformed = [f for f in config['features']['numerical_features'] if f in X.columns]\n",
        "        # Get names for categorical features after OneHotEncoder\n",
        "        cat_features_transformed = preprocessor.named_transformers_['cat'].get_feature_names_out(\n",
        "            [f for f in config['features']['categorical_features'] if f in X.columns]\n",
        "        )\n",
        "        # Combine all feature names\n",
        "        all_feature_names = list(num_features_transformed) + list(cat_features_transformed)\n",
        "    \n",
        "        # Create processed DataFrame\n",
        "        df_processed = pd.DataFrame(X_processed, columns=all_feature_names)\n",
        "        df_processed[target_feature] = y.reset_index(drop=True) # Add target back\n",
        "    \n",
        "        save_dataframe(df_processed, processed_data_path)\n",
        "        joblib.dump(preprocessor, scaler_path) # Save the fitted preprocessor\n",
        "        print(f\"Features built and processed data saved to {processed_data_path}\")\n",
        "        print(f\"Preprocessor saved to {scaler_path}\")\n",
        "    \n",
        "    if __name__ == \"__main__\":\n",
        "        main()\n",
        "    \n",
        "    ```\n",
        "    \n",
        "    -   _Run:_  `python src/features/build_features.py`\n",
        "    -   _Result:_  You should now have  `data/processed/processed_churn_data.csv`  and  `models/scaler.pkl`.\n",
        "\n",
        "### Step 4: More Detailed EDA (using  `notebooks/02_detailed_eda.ipynb`)\n",
        "\n",
        "1.  **Create  `notebooks/02_detailed_eda.ipynb`:**\n",
        "    -   Load  `processed_churn_data.csv`.\n",
        "    -   Perform in-depth visualizations:\n",
        "        \n",
        "        ```\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "        from src.utils.helpers import load_config\n",
        "        import pandas as pd\n",
        "        import os\n",
        "        \n",
        "        # Load configuration\n",
        "        config = load_config()\n",
        "        PROCESSED_DATA_PATH = config['data_paths']['processed']\n",
        "        TARGET_FEATURE = config['features']['target_feature']\n",
        "        FIGURES_DIR = config['output_paths']['figures_dir']\n",
        "        \n",
        "        # Ensure figures directory exists\n",
        "        os.makedirs(FIGURES_DIR, exist_ok=True)\n",
        "        \n",
        "        df_processed = pd.read_csv(PROCESSED_DATA_PATH)\n",
        "        \n",
        "        # Example 1: Churn distribution\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        sns.countplot(x=TARGET_FEATURE, data=df_processed)\n",
        "        plt.title('Churn Distribution')\n",
        "        plt.savefig(os.path.join(FIGURES_DIR, 'churn_distribution.png'))\n",
        "        plt.show()\n",
        "        \n",
        "        # Example 2: MonthlyCharges distribution by Churn\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.histplot(data=df_processed, x='MonthlyCharges', hue=TARGET_FEATURE, kde=True)\n",
        "        plt.title('MonthlyCharges Distribution by Churn')\n",
        "        plt.savefig(os.path.join(FIGURES_DIR, 'monthlycharges_churn_dist.png'))\n",
        "        plt.show()\n",
        "        \n",
        "        # Example 3: Correlation Heatmap (only numerical features for simplicity)\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        corr = df_processed.select_dtypes(include=np.number).corr()\n",
        "        sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "        plt.title('Correlation Heatmap of Numerical Features')\n",
        "        plt.savefig(os.path.join(FIGURES_DIR, 'correlation_heatmap.png'))\n",
        "        plt.show()\n",
        "        \n",
        "        ```\n",
        "        \n",
        "    -   _Result:_  Plots saved in  `reports/figures/`.\n",
        "\n",
        "### Step 5: Model Training & Evaluation (Scripted -  `src/models/train_model.py`)\n",
        "\n",
        "1.  **Create  `src/models/train_model.py`:**\n",
        "    \n",
        "    ```\n",
        "    # src/models/train_model.py\n",
        "    import pandas as pd\n",
        "    import joblib\n",
        "    import json\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "    from src.utils.helpers import load_config\n",
        "    \n",
        "    def evaluate_model(model, X_test, y_test):\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
        "    \n",
        "        metrics = {\n",
        "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "            \"precision\": precision_score(y_test, y_pred),\n",
        "            \"recall\": recall_score(y_test, y_pred),\n",
        "            \"f1_score\": f1_score(y_test, y_pred),\n",
        "        }\n",
        "        if y_prob is not None:\n",
        "            metrics[\"roc_auc\"] = roc_auc_score(y_test, y_prob)\n",
        "    \n",
        "        return metrics\n",
        "    \n",
        "    def main():\n",
        "        config = load_config()\n",
        "        processed_data_path = config['data_paths']['processed']\n",
        "        target_feature = config['features']['target_feature']\n",
        "        model_path = config['output_paths']['model_path']\n",
        "        metrics_path = config['output_paths']['metrics_path']\n",
        "        random_state = config['model_params']['random_state']\n",
        "        test_size = config['model_params']['test_size']\n",
        "    \n",
        "        print(f\"Loading processed data from {processed_data_path}...\")\n",
        "        df_processed = pd.read_csv(processed_data_path)\n",
        "    \n",
        "        X = df_processed.drop(columns=[target_feature])\n",
        "        y = df_processed[target_feature]\n",
        "    \n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=test_size, random_state=random_state, stratify=y\n",
        "        )\n",
        "        print(\"Data split into training and testing sets.\")\n",
        "    \n",
        "        # --- Choose your model (e.g., Logistic Regression) ---\n",
        "        print(\"Training Logistic Regression model...\")\n",
        "        lr_params = config['model_params']['logistic_regression']\n",
        "        model = LogisticRegression(random_state=random_state, **lr_params)\n",
        "        model.fit(X_train, y_train)\n",
        "        print(\"Model trained.\")\n",
        "    \n",
        "        print(\"Evaluating model...\")\n",
        "        metrics = evaluate_model(model, X_test, y_test)\n",
        "        print(\"Evaluation Metrics:\")\n",
        "        for metric, value in metrics.items():\n",
        "            print(f\"- {metric}: {value:.4f}\")\n",
        "    \n",
        "        # Save model and metrics\n",
        "        joblib.dump(model, model_path)\n",
        "        with open(metrics_path, 'w') as f:\n",
        "            json.dump(metrics, f, indent=4)\n",
        "    \n",
        "        print(f\"Model saved to {model_path}\")\n",
        "        print(f\"Metrics saved to {metrics_path}\")\n",
        "    \n",
        "    if __name__ == \"__main__\":\n",
        "        main()\n",
        "    \n",
        "    ```\n",
        "    \n",
        "    -   _Run:_  `python src/models/train_model.py`\n",
        "    -   _Result:_  `models/logistic_regression_model.pkl`  and  `reports/results/metrics.json`.\n",
        "\n",
        "### Step 6: Prediction (Scripted -  `src/models/predict_model.py`)\n",
        "\n",
        "1.  **Create  `src/models/predict_model.py`:**\n",
        "    \n",
        "    ```\n",
        "    # src/models/predict_model.py\n",
        "    import pandas as pd\n",
        "    import joblib\n",
        "    import argparse\n",
        "    from src.utils.helpers import load_config\n",
        "    \n",
        "    def main():\n",
        "        parser = argparse.ArgumentParser(description=\"Predict customer churn.\")\n",
        "        parser.add_argument('--input_data', type=str, required=True,\n",
        "                            help=\"Path to the new data CSV file for prediction.\")\n",
        "        args = parser.parse_args()\n",
        "    \n",
        "        config = load_config()\n",
        "        model_path = config['output_paths']['model_path']\n",
        "        scaler_path = config['output_paths']['scaler_path']\n",
        "        target_feature = config['features']['target_feature']\n",
        "    \n",
        "        print(f\"Loading model from {model_path}...\")\n",
        "        model = joblib.load(model_path)\n",
        "        print(f\"Loading preprocessor from {scaler_path}...\")\n",
        "        preprocessor = joblib.load(scaler_path)\n",
        "    \n",
        "        print(f\"Loading new data from {args.input_data}...\")\n",
        "        new_data_df = pd.read_csv(args.input_data)\n",
        "    \n",
        "        # Assume new_data_df needs similar cleaning as make_dataset.py\n",
        "        # In a real scenario, you'd want a common function or class for this.\n",
        "        # For this example, let's just drop customerID if present for consistency.\n",
        "        if 'customerID' in new_data_df.columns:\n",
        "            new_data_df = new_data_df.drop('customerID', axis=1)\n",
        "        if target_feature in new_data_df.columns:\n",
        "            new_data_df = new_data_df.drop(columns=[target_feature]) # Drop target if present\n",
        "    \n",
        "        # Apply the same preprocessing as used during training\n",
        "        X_new_processed = preprocessor.transform(new_data_df)\n",
        "    \n",
        "        print(\"Making predictions...\")\n",
        "        predictions = model.predict(X_new_processed)\n",
        "        probabilities = model.predict_proba(X_new_processed)[:, 1]\n",
        "    \n",
        "        # Add predictions back to original new data for context\n",
        "        new_data_df['Predicted_Churn'] = predictions\n",
        "        new_data_df['Churn_Probability'] = probabilities\n",
        "    \n",
        "        print(\"\\n--- Predictions ---\")\n",
        "        print(new_data_df[['Churn_Probability', 'Predicted_Churn']].head())\n",
        "        # You might save this output, e.g., new_data_df.to_csv(\"predictions.csv\", index=False)\n",
        "    \n",
        "    if __name__ == \"__main__\":\n",
        "        main()\n",
        "    \n",
        "    ```\n",
        "    \n",
        "    -   _Create Dummy Test Data:_  Create a  `data/raw/new_customer_data.csv`  (can be a few rows from your original data, without the 'Churn' column).\n",
        "    -   _Run:_  `python src/models/predict_model.py --input_data data/raw/new_customer_data.csv`\n",
        "\n",
        "----------\n",
        "\n",
        "This comprehensive setup demonstrates how to build an ML project professionally in VS Code. It encourages modularity, uses configuration files, manages dependencies, and tracks important artifacts. You're now equipped to tackle more complex ML challenges with a robust foundation! Good luck, Captain!"
      ],
      "metadata": {
        "id": "QdsVIVuXDCGZ"
      }
    }
  ]
}